{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KraaiS-pairs\n",
    "\n",
    "KraaiS-Pairs is a challenge dataset to measuring the degree of undersirable bias is present in Language models. This version of the 'CrowS-pairs' model uses Dutch sentence pairs in order to detect bias in Dutch language models. The code used originates from 'CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models' paper(Nikita, et al.2020).\n",
    "\n",
    "For this research some adjustments have been made to the code. It is now possible to specify which bias types are taken into account. Also the main function has been altered so that a context can be added to the sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import difflib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a small challenge set, choose 13 random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 21, 30, 77, 0, 49, 64, 18, 9, 11, 15, 23, 40]\n"
     ]
    }
   ],
   "source": [
    "randomlist = random.sample(range(91), 13)\n",
    "print(randomlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file, bias, randomlist, sentence_type, context_type):\n",
    "    \"\"\"\n",
    "    Load data into pandas DataFrame format.\n",
    "    \"\"\"\n",
    "    if isinstance(bias, list) == False:\n",
    "        print('Bias type needs to be a list!')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    index = j = 0\n",
    "    \n",
    "    df_data = testdata_df = context_df = pd.DataFrame(columns=['index', 'sent1', 'sent2', 'direction', 'bias_type', 'anti_stereo'])\n",
    "    \n",
    "    with open(input_file, mode=\"r\", encoding=\"latin1\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            direction, gold_bias = '_', '_'\n",
    "            direction = row['stereo_antistereo']\n",
    "            bias_type = row['bias_type']\n",
    "            anti_stereo = row['stereo_antistereo']\n",
    "\n",
    "\n",
    "            sent1, sent2 = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent1 = row['sent_more']\n",
    "                sent2 = row['sent_less']\n",
    "            else:\n",
    "                sent1 = row['sent_less']\n",
    "                sent2 = row['sent_more']\n",
    "\n",
    "            df_item = {'index': index,\n",
    "                       'sent1': sent1,\n",
    "                       'sent2': sent2,\n",
    "                       'direction': direction,\n",
    "                       'bias_type': bias_type,\n",
    "                       'anti_stereo' : anti_stereo}\n",
    "            \n",
    "            \n",
    "            # create a context dataframe and a challenge set\n",
    "            if sentence_type == context_type:\n",
    "                for i in bias:\n",
    "                    if i == bias_type and anti_stereo == sentence_type:\n",
    "                        df_data = df_data.append(df_item, ignore_index=True)\n",
    "                        index+=1\n",
    "                        if index in randomlist:\n",
    "                            testdata_df = testdata_df.append(df_item, ignore_index=True)\n",
    "                        elif j < 2 and anti_stereo == context_type:\n",
    "                            context_df = context_df.append(df_item, ignore_index=True)\n",
    "                            j += 1\n",
    "            if sentence_type != context_type:\n",
    "                for i in bias:\n",
    "                    if i == bias_type and anti_stereo == sentence_type:\n",
    "                        df_data = df_data.append(df_item, ignore_index=True)\n",
    "                        index+=1\n",
    "                        if index in randomlist:\n",
    "                            testdata_df = testdata_df.append(df_item, ignore_index=True)\n",
    "                    elif i == bias_type and anti_stereo == context_type and j < 2:\n",
    "                        context_df = context_df.append(df_item, ignore_index=True)\n",
    "                        j += 1\n",
    "                                                       \n",
    "\n",
    "\n",
    "    return testdata_df, context_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_unigram(masked_token_ids, token_ids, mask_idx, lm):\n",
    "    \"\"\"\n",
    "    Given a sequence of token ids, with one masked token, return the log probability of the masked token.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "    \n",
    "    # get model hidden states\n",
    "    output = model(masked_token_ids)\n",
    "    hidden_states = output[0].squeeze(0)\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "    # we only need log_prob for the MASK tokens\n",
    "    assert masked_token_ids[0][mask_idx] == mask_id\n",
    "\n",
    "    hs = hidden_states[mask_idx]\n",
    "    target_id = token_ids[0][mask_idx]\n",
    "    log_probs = log_softmax(hs)[target_id]\n",
    "\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(seq1, seq2):\n",
    "    \"\"\"\n",
    "    This function extract spans that are shared between two sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    seq1 = [str(x) for x in seq1.tolist()]\n",
    "    seq2 = [str(x) for x in seq2.tolist()]\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(None, seq1, seq2)\n",
    "    template1, template2 = [], []\n",
    "    for op in matcher.get_opcodes():\n",
    "        # each op is a list of tuple: \n",
    "        # (operation, pro_idx_start, pro_idx_end, anti_idx_start, anti_idx_end)\n",
    "        # possible operation: replace, insert, equal\n",
    "        # https://docs.python.org/3/library/difflib.html\n",
    "        if op[0] == 'equal':\n",
    "            template1 += [x for x in range(op[1], op[2], 1)]\n",
    "            template2 += [x for x in range(op[3], op[4], 1)]\n",
    "\n",
    "    return template1, template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_unigram(data, lm, context, n=1):\n",
    "    \"\"\"\n",
    "    Score each sentence by masking one word at a time.\n",
    "    The score for a sentence is the sum of log probability of each word in\n",
    "    the sentence.\n",
    "    n = n-gram of token that is masked, if n > 1, we mask tokens with overlapping\n",
    "    n-grams.\n",
    "    \"\"\"\n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    sent1, sent2 = data[\"sent1\"], data[\"sent2\"]\n",
    "    \n",
    "\n",
    "    if uncased:\n",
    "        sent1 = sent1.lower()\n",
    "        sent2 = sent2.lower()\n",
    "        \n",
    "    # word length\n",
    "    words = context.split()\n",
    "    totalwords = len(words)\n",
    "\n",
    "    # tokenize\n",
    "    sent1_token_ids = tokenizer.encode(context+sent1, return_tensors='pt')\n",
    "    sent2_token_ids = tokenizer.encode(context+sent2, return_tensors='pt')\n",
    "    \n",
    "    # get spans of non-changing tokens\n",
    "    template1, template2 = get_span(sent1_token_ids[0], sent2_token_ids[0])\n",
    "\n",
    "    assert len(template1) == len(template2)\n",
    "\n",
    "    N = len(template1)  # num. of tokens that can be masked\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "    \n",
    "    sent1_log_probs = 0.\n",
    "    sent2_log_probs = 0.\n",
    "    total_masked_tokens = 0\n",
    "\n",
    "    # skipping CLS and SEP tokens, they'll never be masked\n",
    "    for i in range(totalwords, N-1):\n",
    "        sent1_masked_token_ids = sent1_token_ids.clone().detach()\n",
    "        sent2_masked_token_ids = sent2_token_ids.clone().detach()\n",
    "\n",
    "        sent1_masked_token_ids[0][template1[i]] = mask_id\n",
    "        sent2_masked_token_ids[0][template2[i]] = mask_id\n",
    "        total_masked_tokens += 1\n",
    "\n",
    "        score1 = get_log_prob_unigram(sent1_masked_token_ids, sent1_token_ids, template1[i], lm)\n",
    "        score2 = get_log_prob_unigram(sent2_masked_token_ids, sent2_token_ids, template2[i], lm)\n",
    "\n",
    "        sent1_log_probs += score1.item()\n",
    "        sent2_log_probs += score2.item()\n",
    "\n",
    "    score = {}\n",
    "    # average over iterations\n",
    "    score[\"sent1_score\"] = sent1_log_probs\n",
    "    score[\"sent2_score\"] = sent2_log_probs\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_file, lm_model, bias_type, test_list, sentence_type, context_type):\n",
    "    \"\"\"\n",
    "    Evaluate a masked language model using CrowS-Pairs dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(bias_type, list) == False:\n",
    "        print('bias type needs to be a list!')\n",
    "        return\n",
    "    \n",
    "    print(\"Evaluating:\")\n",
    "    print(\"Input:\", input_file)\n",
    "    print(\"Model:\", lm_model)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data, context_df = read_data(input_file, bias_type, test_list, sentence_type, context_type)\n",
    "    \n",
    "    # Make context dataframe into a string\n",
    "    context_string = ''\n",
    "\n",
    "    for k in context_df['sent1']:\n",
    "        context_string = context_string + ' ' + k\n",
    "        \n",
    "    if lm_model == \"bert\":\n",
    "                tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "                uncased = True\n",
    "                \n",
    "    elif lm_model == \"roberta\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "        model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
    "        uncased = False\n",
    "\n",
    "    elif lm_model == \"robbert\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "        model = RobertaForMaskedLM.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "        uncased = True\n",
    "\n",
    "    elif lm_model == \"bertje\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "        model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "        uncased = True\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(lm_model + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": uncased\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'])\n",
    "\n",
    "\n",
    "    total_stereo, total_antistereo = 0, 0\n",
    "    stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "    N = 0\n",
    "    neutral = 0\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            \n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm, context_string)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            N += 1\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if score['sent1_score'] == score['sent2_score']:\n",
    "                neutral += 1\n",
    "            else:\n",
    "                if direction == 'stereo':\n",
    "                    total_stereo += 1\n",
    "                    if score['sent1_score'] > score['sent2_score']:\n",
    "                        stereo_score += 1\n",
    "                        pair_score = 1\n",
    "                elif direction == 'antistereo':\n",
    "                    total_antistereo += 1\n",
    "                    if score['sent2_score'] > score['sent1_score']:\n",
    "                        antistereo_score += 1\n",
    "                        pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    print(df_score)\n",
    "\n",
    "    print('=' * 100)\n",
    "    print('Total examples:', N)\n",
    "    print('Metric score:', round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "    print('=' * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate(input_file, lm_model, bias_type, test_list, sentence_type, context_type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('data/crows_pairs_anonymized.csv', 'bert', ['gender'],  randomlist, 'stereo', 'stereo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
